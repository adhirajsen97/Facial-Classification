# Facial-Classification-ML
A neural net to classify facial data on a scale of 1-5 in accordance with the golden ratio

* Dataset Overview

Machine Learning Final Project Report

The dataset used in this project is the SCUT-FBP5500_v2 dataset which includes 350x350 pictures of faces in neutral expressions of 2000 Asian females, 2000 Asian males, 750 Caucasian females and 750 Caucasian males. They are all rated by a set of 60 volunteers on a scale from 1 to 5, 5 being the most attractive. The Asian females and males are mostly taken from social media sites where people publicly post their faces, other sources also include pictures found online.
The Caucasian male and females include various easily recognizable celebrities, this may have distorted the ratings of their attractiveness due to their popularity and the fact that many people already consider them attractive. This may lead to the model only being trained on Asian faces.
It should be noted that this dataset was generated by a group of Chinese researchers using volunteers. It is not disclosed where these volunteers are from or what nationality they are. If the volunteers are only Chinese it could be said that the ratings have a bias in terms of determining what an attractive face looks like. The dataset would not take foreign ideas of what is attractive into mind.
The distribution of the ratings is not evenly distributed, most ratings fall between 2 to 4 with very few examples of 1 and 5 ratings. Some possible solutions to this problem could be subsampling, leaving out common examples, or supersampling, synthetically creating more “rare” examples. SMOTE is a package in R that is capable to doing this.

* Methodology

The labels for the pictures were preprocessed using pandas dataframes. The csv file provided by the researchers in their dataset has the rating of each rater for every picture. This means 60 ratings for every picture, totaling 200,000 total ratings. The average rating (floating point values) of each picture was calculated and then assigned to the corresponding filename. This provided cleaned and preprocessed data for any model.
The images were processed by providing the directory of the folder containing the pictures, looping through each file in the directory was read and resized using the cv2 package, each was added to an array. The array was converted into a numpy array and was scaled to have all pixels between the value of 0 or 1. The images were added in the same relative order as the labels to ensure the appropriate label corresponded to the appropriate image.
Rounded Ratings – Classification
When rating someone’s attractiveness it is typical to assign an integer from 1 to 10, or in the case of this dataset 1 to 5. What isn’t typical is to assign a continuous value, eg. 3.433333, to
rate someone’s attractiveness. In the second approach below we output a continuous value, our first approach was to transform the task into a classification process.

To accomplish this the labels dataframe ratings column was rounded to the nearest integer and was then one-hot encoded to allow for the use of the cross-entropy loss function. Accuracy was used as the metric to evaluate performance. Due to the fact that the original researchers used MAE instead of accuracy for their benchmarks there is nothing to compare the results to. See Experimental results and Analysis for the results of the model.
Continuous Ratings – Regression
Emulating the approach that the original researchers took, the output of the model was changed to give a continuous value. Although not how a typical real life scenario plays out, this was done for the sake of being able to compare to a benchmark. RMAE was used as the loss function and MAE for the metric to evaluate performance.

* Techniques

CNN filters were used to create patters in order to learn which faces are deemed attractive. The models incorporate max pooling in order to reduce dimensionality as the image passes through each layer. After passing through several CNN layers, the image is flattened and passed to a fully connected layer which outputs with either 1 neuron or 5, depending on the approach of the model. Scikit-learn’s cross validation function was used to split the data into training and test sets.

  --- Experimental Results and Analysis ---
  
Experiment 1 - Base model for cross entropy

Parameters:   
              **Neural Net**
              **Activation**: ReLU
              **Loss Function:** Cross Entropy CNN Layers: 6
              **Filters**: (32,32,64,64,128,128) Dropout: 3 (1 for each 2 CNN) Dropout %: 0.25
              **Padding**: “same”
              **Kernel Size**: 3x3 Max Pooling: 3
              **Pool Size**: 2x2
              **Fully Connected Layers**: 2
              **Softmax**: 1 Neurons: (512,5) Dropout: 1 Dropout %: 0.3
              
Results:      
              **Epochs**: 64
              **Train/Test Split**: 75:25
              **Size of Dataset**: 2000 
              **Training Accuracy**: 0.604 
              **Test Accuracy**: 0.449 
              **Training Cross Entropy Sum**: 55.368
              **Testing Cross Entropy Sum**: 88.099
              
Experiment 2 - Increased number of fully connected layers, from 2 to 4.

Parameters:   
              **Neural Net**
              **Activation**: ReLU
              **Loss Function**: Cross Entropy 
              **CNN Layers**: 6
              **Filters**: (32,32,64,64,128,128) Dropout: 3 (1 for each 2 CNN) Dropout %: 0.25
              **Padding**: “same”
              **Kernel Size**: 3x3 Max Pooling: 3
              **Pool Size**: 2x2
              **Fully Connected Layers**: 4
              **Softmax**: 1
              **Neurons**: (512,128,128,5) Dropout: 3
              **Dropout** %: 0.45
              
Results:      
              **Size of Dataset**: 2000 
              **Training Accuracy**: 0.414 
              **Test Accuracy**:0.452 
              **Training Cross Entropy Sum**: 26.318
              **Testing Cross Entropy Sum**: 27.921
              
Experiment 3 - Base model for RMSE.

Parameters:   
              **Neural Net Activation**: ReLU 
              **Loss Function**: RMSE 
              **CNN Layers**: 6
              **Filters**: (32,32,64,64,128,128) 
              **Dropout**: 3 (1 for each 2 CNN) 
              **Dropout** %: 0.25
              **Padding**: “same”
              **Kernel Size**: 3x3 Max Pooling: 3
              **Pool Size**: 2x2
              **Fully Connected Layers**: 2
              **Softmax**: 1 Neurons: (512,5) 
              **Dropout**: 1 Dropout %: 0.3
              
Results:      
              **Epochs**:250
              **Train/Test Split**: 75:25 Size of Dataset: 2000 
              **Metric**:
              **Training MAE**: 0.7
              **Test MAE**:0.842
              **Loss**:
              **Training RMSE avg**:1.048 
              **Testing RMSE avg**:0.846
              
Experiment 4 - Deeper fully connected layer after CNN.

Parameters:   
              **Neural Net Activation**: ReLU 
              **Loss Function**: RMSE 
              **CNN Layers**: 6
              **Filters**: (32,32,64,64,128,128) 
              **Dropout**: 3 (1 for each 2 CNN) 
              **Dropout** %: 0.25
              **Padding**: “same”
              **Kernel Size**: 3x3 
              **Max Pooling**: 3
              **Pool Size:** 2x2
              **Fully Connected Layers**: 4
              **Softmax**: 1
              **Neurons**: (512,128,128,5) 
              **Dropout**: 3
              **Dropout** %: 0.2
              
Results:      
              **Epochs**:30
              **Train/Test Split**: 75:25 
              **Size of Dataset**: 2000 
              **Metric**:
              **Training MAE**: 1.231
              **Test MAE**:0.794
              **Loss**:
              **Training RMSE avg**: 0.982 
              **Testing RMSE avg**: 1.405
              
Experiment 5 - Base model for RMSE with deeper CNN layers instead of fully connected layers.

Parameters:   
              **Neural Net Activation**: ReLU 
              **Loss Function**: RMSE 
              **CNN Layers:** 12
              **Filters**: (32,32,64,64,128,128,256, 256,512,512,1024,1024)
              **Dropout**: 3 (1 for each 2 CNN) 
              **Dropout** %: 0.25
              **Padding**: “same”
              **Kernel Size**: 3x3
              **Max Pooling**: 3 
              **Pool Size**: 2x2
              **Fully Connected Layers**: 2 
              **Softmax**: 1
              **Neurons**: (512,5) 
              **Dropout**: 1
              **Dropout** %: 0.3
              
Results:      
              **Epochs**:64
              **Train/Test Split**: 75:25 
              **Size of Dataset**: 2000 
              **Metric**:
              **Training MAE**: 0.674
              **Test MAE**: 0.579
              **Loss**:
              **Training RMSE avg**: 0.717 
              **Testing RMSE avg:** 0.830
              
Experiment 6 - Deeper CNN layers, trained with less epochs

Parameters:   
              **Neural Net Activation**: ReLU 
              **Loss Function**: RMSE 
              **CNN Layers**: 12
              **Filters**: (32,32,64,64,128,128,256, 256,512,512,1024,1024)
              **Dropout**: 3 (1 for each 2 CNN) 
              **Dropout** %: 0.25
              **Padding**: “same”
              **Kernel Size**: 3x3
              **Max Pooling**: 3 
              **Pool Size**: 2x2
              **Fully Connected Layers**: 2 
              **Softmax**: 1
              **Neurons**: (512,5) 
              **Dropout**: 1
              **Dropout** %: 0.3
              
Results:     
              **Epochs**:10
              **Train/Test Split**: 75:25 
              **Size of Dataset**: 2000 
              **Metric**:
              **Training MAE**: 0.743
              **Test MAE**: 0.871
              **Loss**:
              **Training RMSE avg**: 1.074 
              **Testing RMSE avg**: 0.904


As we can see, the best performing model among all 6 is the model with the one with the deepest level of CNN layers. This model was subject to overfitting, which is why the 6th model was trained with significantly less epochs.
No more experiments were conducted due to the lengthy amount of time it took to train each model. It is worth noting that Tenserflow with CPU was used and not GPU. In the future the GPU package can be installed to get greater training times.

The worst performing models were ironically what the project was set out to do in the beginning. While attempting to make the problem a classification problem, the models would not give good accuracy. It actually performed worse than a random coin toss.
Taking a lesson from the best preforming models the classification models can in future experiments further deepen their CNN layers. This would hopefully increase their accuracy.
Conclusion
The researchers and their approach of using regression allowed them to get good results. A comparison would be welcome to find out how a classification approach would perform to a regression approach.
Overall as a team we are happy with the amount of experiments we ran and experimentation we did. We learned a lot on preparing data and comparison of models in terms of performance.
Contribution of team members
Alejandro was responsible for running and creating many of the experiments and Subhash helped in providing ideas for tweaking parameters. Subhash also helped in processing the original data to something workable.
